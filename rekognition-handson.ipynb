{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57ffb742",
   "metadata": {},
   "source": [
    "## 目次\n",
    "1. [概要](#anchor1)\n",
    "1. [セットアップ](#anchor2)\n",
    "1. [画像のラベル検出](#anchor3)\n",
    "1. [動画のラベル検出](#anchor4)\n",
    "1. [リソースの削除](#anchor5)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1764581e",
   "metadata": {},
   "source": [
    "<a id=\"anchor1\"></a>\n",
    "## 1. 概要\n",
    "Amazon Rekognition では、イメージ分析とビデオ分析をアプリケーションに簡単に追加することができます。Amazon Rekognition API にイメージやビデオを指定するだけで、このサービスによってモノ、人物、テキスト、シーン、アクティビティを識別できます。不適切なコンテンツも検出できます。Amazon Rekognition では、高精度な顔分析、顔の比較、および顔の検索機能も備えています。顔の検出、分析、比較は、ユーザー検証、カタログ作成、人数計数、公共安全など、多岐にわたって活用できます。\n",
    "\n",
    "本ハンズオンではAmazon Rekogntionのラベル検出を用いて、定点カメラを想定した画像・動画から歩行者・車の検出を行います。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869913a6",
   "metadata": {},
   "source": [
    "<a id=\"anchor2\"></a>\n",
    "## 2. セットアップ\n",
    "本ハンズオンでは[3章](#anchor3)で画像を用いたラベル検出、[4章](#anchor4)で動画を用いたラベル検出を行います。以降で利用するライブラリ・各種定数をここで読み込んでおきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb01307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ノートブックの初期化\n",
    "import boto3\n",
    "import cv2\n",
    "import glob\n",
    "from IPython.display import HTML, display, Image as IImage\n",
    "from PIL import Image, ImageDraw, ImageFont, ImageColor\n",
    "import time\n",
    "import os\n",
    "import base64\n",
    "import numpy as np\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8298a9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#　定数を設定\n",
    "# Rekognitionで分析する画像のパスを設定\n",
    "IMAGE_DIR='image'\n",
    "SAMPLE_IMAGE_PATH = os.path.join(IMAGE_DIR, 'sample.jpg')\n",
    "SAMPLE_FORMAT = 'jpg'\n",
    "\n",
    "# Rekognitionで分析する動画のパスを設定\n",
    "MOVIE_DIR='movie'\n",
    "SAMPLE_MOVIE_PATH = os.path.join(MOVIE_DIR, 'sample.mp4')\n",
    "MOVIE_IMAGES_DIR = os.path.join(MOVIE_DIR, 'images')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c65806",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"anchor3\"></a>\n",
    "## 3. 画像のラベル検出\n",
    "本章では画像のラベル検出を行います。Rekogntionでは、物体(花、樹木、テーブルなど)、イベント（結婚式、卒業式、誕生日会など）、概念（風景、夜、自然など）のラベルを画像から検出することができます。Amazon Rekognition ImageのDetectLabels APIを実際に使ってみることで、どのような応答が返ってくるか試してみましょう。まずは以下のコードでrekognitionのClient APIを初期化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787686ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rekognitionのClient APIを初期化\n",
    "rekognition = boto3.client('rekognition')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef108f10",
   "metadata": {},
   "source": [
    "次に、本章でラベル検出する対象の画像を見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d21a0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(IImage(SAMPLE_IMAGE_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d52e7d0",
   "metadata": {},
   "source": [
    "上記の画像をRekognitionで分析し、車・人をラベル検出できるのかを試してみましょう。以下のコードを実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27294d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = cv2.imread(SAMPLE_IMAGE_PATH)\n",
    "hasFrame, imageBytes = cv2.imencode(\".png\", frame)\n",
    "\n",
    "# analyze an image using Amazon Rekognition Custom Labels endpoint\n",
    "detectLabelsResponse = rekognition.detect_labels(\n",
    "    Image={\n",
    "        'Bytes': imageBytes.tobytes()\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528ca5c6",
   "metadata": {},
   "source": [
    "Rekognitionでラベル検出する際は、DetectLabels APIを使用します。APIの詳細については[ドキュメント](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/rekognition.html#Rekognition.Client.detect_labels)を参照下さい。\n",
    "DetectLabels APIから返却された結果を見てみましょう。Person, Car, City, Traffic Lightなど様々なラベルが検出できていることがわかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3afbdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print([list_['Name'] for list_ in detectLabelsResponse['Labels']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b724e5",
   "metadata": {},
   "source": [
    "返却された結果全てを見たい場合は以下のセルを実行してください。各ラベルの名称、信頼度(Confidence)、Bounding Boxの座標情報が返却されていることがわかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e34d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(detectLabelsResponse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5caff0b5",
   "metadata": {},
   "source": [
    "今回は画像から人・車の検出ができるかどうかに焦点を当てます。以下のコードで返却された結果から人(Person)・車(Car)のみを抽出し元画像にラベルを描画してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bbf879",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = list(ImageColor.colormap.values())\n",
    "image_np = np.array(Image.open(SAMPLE_IMAGE_PATH))\n",
    "plt.figure(figsize=(20,20))\n",
    "ax = plt.axes()\n",
    "ax.imshow(image_np)\n",
    "\n",
    "# 描画する対象のリストを作成\n",
    "detect_list = ['Person', 'Car']\n",
    "class_num = len(detect_list)\n",
    "for idx, label in enumerate(detectLabelsResponse['Labels']):\n",
    "    if label['Name'] not in detect_list:\n",
    "        continue\n",
    "    label_name = label['Name']\n",
    "    for instance in label['Instances']:\n",
    "        if instance['Confidence'] < 80:\n",
    "            continue\n",
    "        instance['BoundingBox']\n",
    "        left=instance['BoundingBox']['Left']\n",
    "        top=instance['BoundingBox']['Top']\n",
    "        right=instance['BoundingBox']['Width'] + left\n",
    "        bot=instance['BoundingBox']['Height'] + top\n",
    "        x, w = [val * image_np.shape[1] for val in [left, right - left]]\n",
    "        y, h = [val * image_np.shape[0] for val in [bot, top - bot]]\n",
    "        color = colors[hash(idx) % len(colors)]\n",
    "        rect = patches.Rectangle((x, y), w, h, linewidth=3, edgecolor=color, facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x, y, '{} {:.0f}%'.format(label_name, instance['Confidence']), bbox=dict(facecolor='white', alpha=0.5))        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c38f947",
   "metadata": {},
   "source": [
    "上記の結果から、画像内に存在している人・車をラベル検出できていることがわかります。Rekognitionがサポートしているラベルのフルセットは[こちら](https://d2aslwxpztrh8x.cloudfront.net/1d2f4f70-e269-4194-9bee-c4b31eb99266/AmazonRekognitionLabels_v2.0.zip)からダウンロードできます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3b446b",
   "metadata": {},
   "source": [
    "<a id=\"anchor4\"></a>\n",
    "## 4. 動画のラベル検出"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7a967a",
   "metadata": {},
   "source": [
    "本章では動画のラベル検出を行います。本ハンズオンでは、動画(mp4)を連番静止画に分割し、各画像に対して推論を実行、その結果を画像に重畳して最後に動画へ変換します。それでは以下のコードを実行し、rekognitionのClient APIを初期化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5227e503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rekognitionのClient APIを初期化\n",
    "rekognition = boto3.client('rekognition')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1953baa",
   "metadata": {},
   "source": [
    "次に、/movie下に格納されているsample.mp4を読み込み、連番の静止画に分割します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee96cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def divideVideo(video_path, img_path):\n",
    "    '''動画を静止画に分割する\n",
    "    \n",
    "    Args:\n",
    "        video_path(str): 分割対象の動画のパス\n",
    "        img_path: 分割した静止画の保存先パス\n",
    "    \n",
    "    Returns:\n",
    "        \n",
    "    \n",
    "    '''\n",
    "\n",
    "    # 静止画の保存先がまだ存在していなかった場合は作成\n",
    "    if not os.path.isdir(img_path):\n",
    "        os.mkdir(img_path)\n",
    "\n",
    "    # 動画の読み込み\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # 動画を分割しPNG形式で保存\n",
    "    while(cap.isOpened()):\n",
    "        frameId = cap.get(1) #current frame number\n",
    "        print(\"Processing frame id: {}\".format(frameId))\n",
    "        ret, frame = cap.read()\n",
    "        if (ret != True):\n",
    "            break\n",
    "        img_name = 'image_' + str(int(frameId)).zfill(4) + '.png'\n",
    "        path = os.path.join(img_path, img_name)\n",
    "        cv2.imwrite(path, frame)\n",
    "    \n",
    "    cap.release()\n",
    "\n",
    "divideVideo(SAMPLE_MOVIE_PATH, MOVIE_IMAGES_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c74ab32",
   "metadata": {},
   "source": [
    "上記で分割した静止画を順にラベル検出します。ラベル検出は[3章](#anchor3)と同様にDetectLabels APIを用いて行います。静止画についてラベル検出を行ったあと、OpenCVを用いてフレーム処理を行い、Bounding Boxを描画した動画を作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebe9c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyzeVideo(movie_path, img_path, analyzed_movie_path, detect):\n",
    "    '''動画を解析する\n",
    "    \n",
    "    Args:\n",
    "        movie_path(str): 解析対象の動画の保存先パス\n",
    "        img_path(str): 分割した静止画の保存先パス\n",
    "        analyzed_movie_path(str): 分析した動画の保存先パス\n",
    "        detect (list) :検出したい対象のリスト\n",
    "    \n",
    "    '''\n",
    "    # set movie's information\n",
    "    mv= cv2.VideoCapture(movie_path)#動画の読み込み\n",
    "    frame_count =int(mv.get(cv2.CAP_PROP_FRAME_COUNT))#動画のフレームをすべて取得\n",
    "    frame_rate = int(mv.get(cv2.CAP_PROP_FPS))#読み込んだ動画のFPS(フレームレート)を調べる\n",
    "    movie_width = int(mv.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    movie_height = int(mv.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    # settings for make video file\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    writer = cv2.VideoWriter(analyzed_movie_path, fourcc, frame_rate, (movie_width, movie_height))\n",
    "    \n",
    "    rekognition = boto3.client('rekognition')\n",
    "    file_list = sorted(glob.glob(os.path.join(img_path, '*')))\n",
    "\n",
    "    # analyze each image\n",
    "    for i, d in enumerate(file_list):\n",
    "        frameId = i+1 #current frame number\n",
    "        print(\"Processing frame id: {}\".format(frameId))\n",
    "        frame = cv2.imread(d)\n",
    "        #print(d)\n",
    "        hasFrame, imageBytes = cv2.imencode(\".png\", frame)\n",
    "        \n",
    "        # ラベル検出\n",
    "        response = rekognition.detect_labels(\n",
    "            Image={\n",
    "                'Bytes': imageBytes.tobytes()\n",
    "            }\n",
    "        )\n",
    "        # draw rectangles on an image using the analysis results\n",
    "        for output in response[\"Labels\"]:\n",
    "        #    print(output)\n",
    "            Name = output['Name']\n",
    "            if Name not in detect:\n",
    "                continue\n",
    "\n",
    "            Confidence = output['Confidence']\n",
    "            for instance in output['Instances']:\n",
    "                w = instance['BoundingBox']['Width']\n",
    "                h = instance['BoundingBox']['Height']\n",
    "                left = instance['BoundingBox']['Left']\n",
    "                top = instance['BoundingBox']['Top']\n",
    "                w = int(w * movie_width)\n",
    "                h = int(h * movie_height)\n",
    "                left = int(left*movie_width)\n",
    "                top = int(top*movie_height)\n",
    "\n",
    "                cv2.rectangle(frame,(left,top),(left+w,top+h),(0,0,255),2)\n",
    "                cv2.rectangle(frame,(left,top-20),(left+len(Name)*8, top),(0,0,255),-1)\n",
    "                cv2.putText(frame, Name + ': ' , (left, top-10), cv2.FONT_HERSHEY_SIMPLEX, 0.40,(255, 255, 255), 2)                    \n",
    "        writer.write(frame)\n",
    "\n",
    "    writer.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# 人・車のみを検出するためリストを作成\n",
    "detect_list = ['Person', 'Car']\n",
    "analyzeVideo(SAMPLE_MOVIE_PATH, MOVIE_IMAGES_DIR, 'result.mp4', detect_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10823c09",
   "metadata": {},
   "source": [
    "上記のコードを実行すると、同一階層にresult.mp4が生成されているので、確認してみましょう。動画でも問題なくラベル検出ができていることがわかります。今回は動画を静止画に分割し、Amazon Rekognitionでラベル検出を行なう手法をご紹介しました。この手法はリアルタイムに動画を分析する際によく使われる手法です。リアルタイムな動画分析の場合、ラベル検出はRekognitionではなくSageMakerで独自に学習モデルを作成してデプロイした推論エンドポイントに対しても行われることもあります（参考：[Amazon Kinesis Video Streams および Amazon SageMaker を使用したリアルタイムでのライブビデオの分析](https://aws.amazon.com/jp/blogs/news/analyze-live-video-at-scale-in-real-time-using-amazon-kinesis-video-streams-and-amazon-sagemaker/)）。\n",
    "上記手法のほか、Amazon Rekognition Videoを用いても動画のラベル検出を行うことができます。Amazon Rekogntion Videoは、Amazon S3 に保存されているビデオから、物体、シーン、有名人、テキスト、動作、および不適切なコンテンツを検出する、機械学習を利用したビデオ分析サービスです。また、Rekognition Video は精度の高い顔分析および顔検索機能も備えており、顔の検出、分析、比較が行えるほか、ビデオ内での人物の動きを追跡することもできます。Amazon Rekognition Videoで動画のラベル検出をするとどのような結果が返ってくるのか確認してみましょう。Amazon Rekognition Videoを用いるには、分析したい対象の動画をS3バケットに格納する必要があります。まずは以下のコードを実行してS3バケットを作成し、分析したい動画を格納しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40530ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "S3_BUCKET = 'rekognition-video-handson' + str(uuid.uuid4()) # 一意の文字列にするためUUIDを付与\n",
    "region = boto3.session.Session().region_name\n",
    "location = {'LocationConstraint': region}\n",
    "s3_client = boto3.client('s3')\n",
    "response = s3_client.create_bucket(Bucket=S3_BUCKET,CreateBucketConfiguration=location)\n",
    "s3_client.upload_file(SAMPLE_MOVIE_PATH, S3_BUCKET, SAMPLE_MOVIE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a41580",
   "metadata": {},
   "source": [
    "上記でS3に格納したパスを指定して、Rekognition Videoでのラベル検出を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8786f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rekognition.start_label_detection(\n",
    "    Video={'S3Object': \n",
    "           {'Bucket': S3_BUCKET, \n",
    "            'Name': SAMPLE_MOVIE_PATH\n",
    "           }\n",
    "          }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ace6f3",
   "metadata": {},
   "source": [
    "下記のコードを実行し、分析の状態がSUCCEEDEDとなるまで待ちます。IN_PROGRESSと出力された場合は、少し時間を置いてから再度上記のコードを実行してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef6fd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = rekognition.get_label_detection(JobId=response['JobId'])\n",
    "print(result['JobStatus'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec827a9",
   "metadata": {},
   "source": [
    "上記のコードを実行しSUCCEEDEDと表示されたら、動画のラベル検出は終了しています。GetLabelDetection APIから返却された結果を見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcddb0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf08a14",
   "metadata": {},
   "source": [
    "結果を見ると、時間(Timestamp)ごとにラベル検出されていることがわかります。本ハンズオンでは割愛しますが、この結果を用いて動画に対してフレーム処理を行うことで、Bounding Boxを描画した動画を作成することができます。\n",
    "GetLabelDetection APIからのレスポンスの詳細は[ドキュメント](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/rekognition.html#Rekognition.Client.get_label_detection)を参照してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de607a74",
   "metadata": {},
   "source": [
    "<a id=\"anchor5\"></a>\n",
    "## 5. リソースの削除\n",
    "本ハンズオンで作成したリソースの削除を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460d4730",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = s3_client.delete_object(Bucket=S3_BUCKET,Key=SAMPLE_MOVIE_PATH)\n",
    "res = s3_client.delete_bucket(Bucket=S3_BUCKET)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
